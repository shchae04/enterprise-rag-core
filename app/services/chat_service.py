import os
from typing import List, Tuple
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.embedding import Embedding
from app.models.document import Document
from app.services.vector_service import VectorService
from app.services.rerank_service import KeywordReranker
from app.core.config import settings
from app.core.logging import logger
import google.generativeai as genai

class ChatService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.vector_service = VectorService(db) # Inject VectorService
        # ì¶”í›„ ì„¤ì •ê°’ì— ë”°ë¼ CohereReranker ë“±ìœ¼ë¡œ êµì²´ ê°€ëŠ¥
        self.reranker = KeywordReranker(boost_weight=0.3) 

    async def get_answer(self, query: str, k: int = 4) -> Tuple[str, List[dict]]:
        # 0. Query Expansion (ì–´íœ˜ ë¶ˆì¼ì¹˜ í•´ê²°)
        expanded_query = await self._expand_query(query)

        # 1. Hybrid Search (Vector + Keyword -> RRF)
        # ì´ì œ VectorService.search_hybridê°€ RRFëœ ìƒìœ„ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        # ë‚´ë¶€ì ìœ¼ë¡œ k * 4ê°œë¥¼ í›„ë³´ë¡œ ë½‘ì•„ì„œ RRF í›„ ìƒìœ„ k * 2ê°œ ì •ë„ë¥¼ ë¦¬í„´ë°›ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜,
        # search_hybrid ì¸í„°í˜ì´ìŠ¤ê°€ top_kë¥¼ ë°›ìœ¼ë¯€ë¡œ, ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•´ ë„‰ë„‰íˆ k * 3ê°œë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
        rrf_limit = k * 3
        
        # Note: search_hybrid returns List[Embedding] (entity objects)
        # We need to fetch associated Document metadata.
        # Ideally search_hybrid should return tuples or we lazy load.
        # But Embedding has document relationship (lazy loading in async might be tricky without specific loader options).
        # Let's verify search_hybrid implementation. It returns Embedding scalars.
        # Accessing embedding.document might trigger lazy load error in async unless we use joinedload or explicit join.
        
        # For performance and correctness, let's modify how we call or use the results.
        # Actually, VectorService methods just execute simple selects.
        # We might need to fetch Document info.
        
        # Let's assume Embedding object has what we need or we fetch documents by ID.
        candidate_embeddings = await self.vector_service.search_hybrid(expanded_query, top_k=rrf_limit)
        
        if not candidate_embeddings:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

        # Fetch Documents for candidates
        # To avoid N+1, let's fetch documents in batch
        doc_ids = list(set([e.document_id for e in candidate_embeddings]))
        stmt = select(Document).where(Document.id.in_(doc_ids))
        result = await self.db.execute(stmt)
        documents_map = {doc.id: doc for doc in result.scalars().all()}

        # 3. Prepare for Final Reranking (Cross-Check or Keyword Boosting again)
        # Hybrid Search already did RRF (which includes keyword match).
        # But our KeywordReranker (in rerank_service.py) might do specific scoring or we can skip it?
        # Hybrid Search (RRF) IS a form of reranking.
        # If we trust RRF, we can just take top K.
        # However, ChatService architecture uses KeywordReranker which might use different logic (e.g. precise exact match boosting).
        # Let's keep it for fine-tuning.
        
        candidate_input = []
        # Since RRF score is not directly attached to embedding object in the return list (it just returns list),
        # we lose the score information unless we modify search_hybrid to return scores.
        # Current search_hybrid returns list of Embeddings.
        # Let's assume order is importance.
        # We can assign mock score based on rank.
        
        for rank, embedding in enumerate(candidate_embeddings):
            doc = documents_map.get(embedding.document_id)
            if not doc: continue
            
            # Reverse rank score (higher is better)
            mock_score = 1.0 / (rank + 1)
            
            candidate_input.append({
                "document_id": str(doc.id),
                "content": embedding.content,
                "score": mock_score,
                "filename": doc.filename
            })

        # 4. Final Reranking (Optional but good for robustness)
        # Using the original query for reranking to focus on user intent (not expanded one)
        # or use expanded? Usually original is better for precision.
        reranked_results = await self.reranker.rerank(query, candidate_input)
        
        # 5. Top-K Slice
        final_top_k = reranked_results[:k]

        # 6. Construct Context
        context_texts = []
        sources = []
        for res in final_top_k:
            context_texts.append(res.content)
            sources.append({
                "document_id": res.document_id,
                "filename": res.filename,
                "content": res.content[:200] + "...",
                "score": res.score
            })

        context = "\n\n".join(context_texts)

        # 7. Generate Answer using LLM
        answer = await self._generate_llm_response(query, context)
        
        return answer, sources

    async def _expand_query(self, query: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ë™ì˜ì–´/ìœ ì‚¬ì–´ë¥¼ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
        """
        try:
            model = genai.GenerativeModel(settings.LLM_MODEL)
            prompt = f"""
ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§„ ìœ ì‚¬ì–´, ë™ì˜ì–´, ê´€ë ¨ ìš©ì–´ë¥¼ í¬í•¨í•˜ì—¬ í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ë‹¨, ì›ë³¸ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}
í™•ì¥ëœ ì¿¼ë¦¬ (ì½¤ë§ˆë¡œ êµ¬ë¶„):
"""
            response = await model.generate_content_async(prompt)
            expanded = response.text.strip()
            logger.info(f"ğŸ” Query Expansion: '{query}' â†’ '{expanded}'")
            return expanded
        except Exception as e:
            logger.warning(f"âš ï¸ Query expansion failed, using original query: {e}")
            return query  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¿¼ë¦¬ ë°˜í™˜

    async def _generate_llm_response(self, query: str, context: str) -> str:
        try:
            prompt = f"""
            ë‹¹ì‹ ì€ ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œ ê¸°ë°˜ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ì•„ë˜ ì œê³µëœ [ë¬¸ì„œ ë‚´ìš©]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ì„¸ìš”.
            ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.

            [ë¬¸ì„œ ë‚´ìš©]
            {context}

            [ì§ˆë¬¸]
            {query}
            
            ë‹µë³€:
            """

            if settings.LLM_PROVIDER == "gemini":
                model = genai.GenerativeModel(settings.LLM_MODEL)
                response = await model.generate_content_async(prompt)
                return response.text
                
            elif settings.LLM_PROVIDER == "openai":
                from openai import AsyncOpenAI
                client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                response = await client.chat.completions.create(
                    model=settings.LLM_MODEL,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.choices[0].message.content

            elif settings.LLM_PROVIDER == "anthropic":
                from anthropic import AsyncAnthropic
                client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
                response = await client.messages.create(
                    model=settings.LLM_MODEL,
                    max_tokens=1024,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text

            else:
                return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM Providerì…ë‹ˆë‹¤: {settings.LLM_PROVIDER}"

        except Exception as e:
            logger.error(f"Error generating answer: {e}", exc_info=True)
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."