import asyncio
import os
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
# For Gemini support in Ragas, we might need custom LLM/Embeddings config
# But for simplicity, we assume OPENAI_API_KEY is present or we configure Ragas to use Gemini via LangChain wrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from app.services.chat_service import ChatService
from app.core.database import AsyncSessionLocal
from app.core.config import settings

# Test Data (Golden Dataset)
TEST_QUESTIONS = [
    {
        "question": "RAG ì‹œìŠ¤í…œì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ground_truth": "RAG ì‹œìŠ¤í…œì€ ì¿¼ë¦¬ í™•ì¥, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ì¬ìˆœìœ„í™” íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ë¬¸ì„œ í¬ë§·ì„ ì§€ì›í•˜ê³  ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì•„í‚¤í…ì²˜ë¥¼ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤."
    },
    {
        "question": "ì§€ì›í•˜ëŠ” ë¬¸ì„œ í¬ë§·ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
        "ground_truth": "HWP, PDF, DOCX, XLSX, TXT, MD ë“± ì£¼ìš” ë¬¸ì„œ í¬ë§·ì„ ì§€ì›í•©ë‹ˆë‹¤."
    },
    {
        "question": "ë°±ì—”ë“œ ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ground_truth": "ë°±ì—”ë“œëŠ” FastAPI, PostgreSQL(pgvector), SQLAlchemy Async, Celery, Redis ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    }
]

async def generate_rag_responses():
    """
    Generate answers using our actual RAG pipeline
    """
    results = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }

    print("ğŸš€ Generating responses from RAG pipeline...")
    
    async with AsyncSessionLocal() as db:
        chat_service = ChatService(db)
        
        for item in TEST_QUESTIONS:
            query = item["question"]
            ground_truth = item["ground_truth"]
            
            # Call actual RAG
            answer, sources = await chat_service.get_answer(query)
            
            # Extract contexts
            context_texts = [s["content"] for s in sources]
            
            results["question"].append(query)
            results["answer"].append(answer)
            results["contexts"].append(context_texts)
            results["ground_truth"].append(ground_truth)
            
    return results

def run_evaluation():
    # 1. Generate Responses
    data_dict = asyncio.run(generate_rag_responses())
    dataset = Dataset.from_dict(data_dict)

    # 2. Configure Ragas with Gemini (if available) or OpenAI
    # If OPENAI_API_KEY is not set, this might fail unless we configure Gemini
    # Here we assume OpenAI key is available for evaluation (standard practice)
    # or we can wrap Gemini.
    
    print("ğŸ“Š Running Ragas evaluation...")
    
    # Define metrics
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
    ]

    # Run evaluation
    results = evaluate(
        dataset=dataset,
        metrics=metrics,
    )

    # 3. Print Results
    print("\n================ RAG EVALUATION REPORT ================")
    print(results)
    df = results.to_pandas()
    print("\nDetailed Results:")
    print(df[['question', 'answer', 'faithfulness', 'answer_relevancy']])
    
    # Save report
    os.makedirs("tests/reports", exist_ok=True)
    df.to_csv("tests/reports/ragas_report.csv", index=False)
    print(f"\nâœ… Report saved to tests/reports/ragas_report.csv")

if __name__ == "__main__":
    run_evaluation()
